{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Classification Project - NYC Cycling Fatalities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem and Overview\n",
    "\n",
    "   Our proposed client is the City of New York itself. The city government has come under fire from constituents and cyclist advocacy groups for the dangerous conditions cyclists endure. The city's transportation leadership want to know what conditions lead to deadly traffic collisions for New York's cyclists. The classification model in this instance is not the end product, as once the city has a record of the accident, they already know if a cyclist has died or not. The final product of this analysis will be the emergent patters revealed by what parameters make for a classification model capable of predicting fatal cycling accidents.\n",
    "   \n",
    "   Fittingly, the City of New York is also the source of our data. Our dataset comes in the form of 1.8 million rows detailing motor vehicle collisions that have occured in NYC. The city publishes this data via the NYC Open Data project, an initiative undertaken by the city to make publicly aggregated data accessible to citizens. The data spans nearly a decade, covering dates from July 2012, and is updated almost daily. \n",
    "   \n",
    "   Our dataset is highly imbalanced (99.5% - .5%) which means that using accuracy score as a default would be a mistake, as even a dummy model that simply guessed the majority class every time would have an accuracy of 99.5%. For our purposes, a metric such as recall would work much better, as judging our model on its ability to find the true positives (lethal collisions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the Relevant Libraries, Modules, and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# This pandas option makes sure the juptyer notebook displays all the columns \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "seed = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CSV Into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we read our csv into a pandas dataframe. This can take a while. \n",
    "df = pd.read_csv('../data/Motor_Vehicle_Collisions.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1830092 entries, 0 to 1830091\n",
      "Data columns (total 29 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   CRASH DATE                     object \n",
      " 1   CRASH TIME                     object \n",
      " 2   BOROUGH                        object \n",
      " 3   ZIP CODE                       object \n",
      " 4   LATITUDE                       float64\n",
      " 5   LONGITUDE                      float64\n",
      " 6   LOCATION                       object \n",
      " 7   ON STREET NAME                 object \n",
      " 8   CROSS STREET NAME              object \n",
      " 9   OFF STREET NAME                object \n",
      " 10  NUMBER OF PERSONS INJURED      float64\n",
      " 11  NUMBER OF PERSONS KILLED       float64\n",
      " 12  NUMBER OF PEDESTRIANS INJURED  int64  \n",
      " 13  NUMBER OF PEDESTRIANS KILLED   int64  \n",
      " 14  NUMBER OF CYCLIST INJURED      int64  \n",
      " 15  NUMBER OF CYCLIST KILLED       int64  \n",
      " 16  NUMBER OF MOTORIST INJURED     int64  \n",
      " 17  NUMBER OF MOTORIST KILLED      int64  \n",
      " 18  CONTRIBUTING FACTOR VEHICLE 1  object \n",
      " 19  CONTRIBUTING FACTOR VEHICLE 2  object \n",
      " 20  CONTRIBUTING FACTOR VEHICLE 3  object \n",
      " 21  CONTRIBUTING FACTOR VEHICLE 4  object \n",
      " 22  CONTRIBUTING FACTOR VEHICLE 5  object \n",
      " 23  COLLISION_ID                   int64  \n",
      " 24  VEHICLE TYPE CODE 1            object \n",
      " 25  VEHICLE TYPE CODE 2            object \n",
      " 26  VEHICLE TYPE CODE 3            object \n",
      " 27  VEHICLE TYPE CODE 4            object \n",
      " 28  VEHICLE TYPE CODE 5            object \n",
      "dtypes: float64(4), int64(7), object(18)\n",
      "memory usage: 404.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Seperating by Cyclist Fatalities, Cyclist Injuries and Finding Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets start by finding how many collisions involved a cyclist, fatally or otherwise. \n",
    "cyclist_collisions = df.loc[ (df['NUMBER OF CYCLIST INJURED'] > 0) | (df['NUMBER OF CYCLIST KILLED'] > 0)]\n",
    "len(cyclist_collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This df is comprised of collisions where one or more cyclist were injured, but none died. \n",
    "non_lethal_collisions = df.loc[(df['NUMBER OF CYCLIST INJURED'] > 0) & (df['NUMBER OF CYCLIST KILLED'] == 0)]\n",
    "len(non_lethal_collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next lets find how many rows record a cyclist dying. \n",
    "lethal_collisions = df.loc[df['NUMBER OF CYCLIST KILLED'] > 0]\n",
    "len(lethal_collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are determining how many collisions resulted in both cyclist injuries, and cyclist fatalities. \n",
    "combination_collisions = df.loc[ (df['NUMBER OF CYCLIST INJURED'] > 0) & (df['NUMBER OF CYCLIST KILLED'] > 0) ]\n",
    "len(combination_collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>NUMBER OF PERSONS INJURED</th>\n",
       "      <th>NUMBER OF PERSONS KILLED</th>\n",
       "      <th>NUMBER OF PEDESTRIANS INJURED</th>\n",
       "      <th>NUMBER OF PEDESTRIANS KILLED</th>\n",
       "      <th>NUMBER OF CYCLIST INJURED</th>\n",
       "      <th>NUMBER OF CYCLIST KILLED</th>\n",
       "      <th>NUMBER OF MOTORIST INJURED</th>\n",
       "      <th>NUMBER OF MOTORIST KILLED</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 1</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687327</th>\n",
       "      <td>10/31/2017</td>\n",
       "      <td>15:08</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>10014</td>\n",
       "      <td>40.729046</td>\n",
       "      <td>-74.01073</td>\n",
       "      <td>(40.729046, -74.01073)</td>\n",
       "      <td>WEST STREET</td>\n",
       "      <td>WEST HOUSTON STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Other Vehicular</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>3782508</td>\n",
       "      <td>Flat Bed</td>\n",
       "      <td>Bus</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Bike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRASH DATE CRASH TIME    BOROUGH ZIP CODE   LATITUDE  LONGITUDE  \\\n",
       "687327  10/31/2017      15:08  MANHATTAN    10014  40.729046  -74.01073   \n",
       "\n",
       "                      LOCATION                    ON STREET NAME  \\\n",
       "687327  (40.729046, -74.01073)  WEST STREET                        \n",
       "\n",
       "          CROSS STREET NAME OFF STREET NAME  NUMBER OF PERSONS INJURED  \\\n",
       "687327  WEST HOUSTON STREET             NaN                       12.0   \n",
       "\n",
       "        NUMBER OF PERSONS KILLED  NUMBER OF PEDESTRIANS INJURED  \\\n",
       "687327                       8.0                              7   \n",
       "\n",
       "        NUMBER OF PEDESTRIANS KILLED  NUMBER OF CYCLIST INJURED  \\\n",
       "687327                             6                          1   \n",
       "\n",
       "        NUMBER OF CYCLIST KILLED  NUMBER OF MOTORIST INJURED  \\\n",
       "687327                         2                           4   \n",
       "\n",
       "        NUMBER OF MOTORIST KILLED CONTRIBUTING FACTOR VEHICLE 1  \\\n",
       "687327                          0               Other Vehicular   \n",
       "\n",
       "       CONTRIBUTING FACTOR VEHICLE 2 CONTRIBUTING FACTOR VEHICLE 3  \\\n",
       "687327                   Unspecified                   Unspecified   \n",
       "\n",
       "       CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5  \\\n",
       "687327                   Unspecified                   Unspecified   \n",
       "\n",
       "        COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2  \\\n",
       "687327       3782508            Flat Bed                 Bus   \n",
       "\n",
       "       VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5  \n",
       "687327                Bike                Bike                Bike  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only one row details an event where more than one cyclist died.\n",
    "print(len(df.loc[df['NUMBER OF CYCLIST KILLED'] > 1]))\n",
    "\n",
    "# This row describes the tragic terrorist attack that occured on Halloween 2017.\n",
    "df.loc[df['NUMBER OF CYCLIST KILLED'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Counts Analysis \n",
    "    \n",
    "   So out of nearly 2 million recorded traffic collisions, 43,280 involved a cyclist either getting injured, or killed. \n",
    "   \n",
    "   Of those 43,280 collisions involving a cyclist, 43,101 detail a cyclist getting injured, but no cyclists dying. This means that in 99.5% of traffic collisions where a cyclist is either injured or killed, the cyclist does not die of their injuries. This is a HIGHLY imbalanced dataset, and our classifiers will need to be extremely robust to detect the minority class. \n",
    "   \n",
    "   We found 179 rows detailing an incident that resulted in the death of a cyclist. Only 7 rows fell into the category of 'combination collisions' where there were both cyclist injuries, as well as cyclist deaths. These combination rows will be considered lethal collisions in our classification, as they have obvious crossed the threshold for what defines a deadly collision (more than 0 cyclist deaths). \n",
    "   \n",
    "   Only one row in our entire dataset represents an event where more than one cyclist died. This row details the terrorist attack that occured on Halloween of 2017, where an attacker drove a pickup truck down the westside bike path, injuring 12, and killing 8. This tragic event is certainly an outlier in our dataset, and will not be included in our model building process, as anti-terrorist measures on seperated, dedicated bike paths is outside the scope of this project's analysis. In addition, measures to prevent this kind of event from happening again on the west side bike path have already been taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43279"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolating just the collisions involving cyclists\n",
    "cyc_col = df.loc[ (df['NUMBER OF CYCLIST INJURED'] > 0) | (df['NUMBER OF CYCLIST KILLED'] > 0)]\n",
    "\n",
    "# Dropping the 2017 terror attack from our dataframe\n",
    "all_cycling_collisions = cyc_col.loc[cyc_col['NUMBER OF CYCLIST KILLED'] < 2]\n",
    "\n",
    "# Making sure we don't have any collisions recorded twice, by dropping any duplicates in the ID column \n",
    "all_cycling_collisions.drop_duplicates(['COLLISION_ID'])\n",
    "\n",
    "# Checking the length \n",
    "len(all_cycling_collisions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saved our cycling collisions data as its own csv, as this is really the raw dataset of interest. \n",
    "\n",
    "#all_cycling_collisions.to_csv('Cycling_Collisions_Isolated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Column Data and NaN Value Counts\n",
    "\n",
    "   The first problem with much of the information in this dataset, is that it is stored in messy, irregular strings. For example under vehicle type, we see 'taxi' and 'Taxi' as seperate categories. In the date column, the dates are stored as strings, which will have to be converted, or at the very least one hot encoded into buckets if we want date to be a useful feature. \n",
    "   \n",
    "   The other challenge standing in the way of this analysis is going to be dealing with all the NaN values in columns that we need for our models. With this is mind, there are two reasons why we chose to not simply drop rows with NaN values in important input columns. \n",
    "\n",
    "   One is that because our dataset is so incredibly imbalanced, NaN values in important columns, in rows belonging to our minority class, will significantly impede our attempt to generate a generalizable model that can identify the minority class in new unseen data. The other reason we must find a way to deal with the rows containing NaNs is that since the number of lethal collisions is so proportionally small, if we just dropped rows with NaNs in the relevant columns, we would be thinning out our already tiny minority class. \n",
    "   \n",
    "   This means that our process is going to involve a lot of preprocessing, filling in the missing pieces. The first step in that process is to see what the damage is, and look at the NaN value counts for our columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Time and Location Data Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CRASH DATE column is 0.0% NaNs\n",
      "The CRASH TIME column is 0.0% NaNs\n",
      "The BOROUGH column is 21.75651008572287% NaNs\n",
      "The ZIP CODE column is 21.75882067515423% NaNs\n",
      "The LATITUDE column is 7.48168857875644% NaNs\n",
      "The LONGITUDE column is 7.48168857875644% NaNs\n",
      "The LOCATION column is 7.48168857875644% NaNs\n",
      "The ON STREET NAME column is 14.358002726495528% NaNs\n",
      "The CROSS STREET NAME column is 27.325030615309963% NaNs\n",
      "The OFF STREET NAME column is 85.80142794426858% NaNs\n"
     ]
    }
   ],
   "source": [
    "# Displaying the time and location columns \n",
    "for i in all_cycling_collisions.columns[:10]:\n",
    "    print(f'The {i} column is {all_cycling_collisions[i].isna().sum() / len(all_cycling_collisions) * 100}% NaNs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Contributing Factor Columns\n",
    "\n",
    "These columns contain data regarding the cause of the accident, which could be critical for our model. We see that after the second column there is a drastic increase in NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CONTRIBUTING FACTOR VEHICLE 1 column is 0.04159060976455094% NaNs\n",
      "The CONTRIBUTING FACTOR VEHICLE 2 column is 7.125857806326394% NaNs\n",
      "The CONTRIBUTING FACTOR VEHICLE 3 column is 97.70789528408696% NaNs\n",
      "The CONTRIBUTING FACTOR VEHICLE 4 column is 99.75276693084406% NaNs\n",
      "The CONTRIBUTING FACTOR VEHICLE 5 column is 99.91912936990227% NaNs\n"
     ]
    }
   ],
   "source": [
    "# Lets examine the contributing factor columns \n",
    "for i in all_cycling_collisions.columns[18:23]:\n",
    "    print(f'The {i} column is {all_cycling_collisions[i].isna().sum() / len(all_cycling_collisions) * 100}% NaNs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Vehicle Type Code Columns\n",
    "\n",
    "   These 5 columns contain data about the vehicles that were involved in the collision, including the Bike/E-Bike. Vehicle type could be a super important feature for our models, so we will have to deal with the messy string values, as well as NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VEHICLE TYPE CODE 1 column is 0.046211788627278816% NaNs\n",
      "The VEHICLE TYPE CODE 2 column is 9.08754823355438% NaNs\n",
      "The VEHICLE TYPE CODE 3 column is 97.73331176783198% NaNs\n",
      "The VEHICLE TYPE CODE 4 column is 99.75507752027542% NaNs\n",
      "The VEHICLE TYPE CODE 5 column is 99.91450819103953% NaNs\n"
     ]
    }
   ],
   "source": [
    "# Displaying the vehicle type code columns\n",
    "for i in all_cycling_collisions.columns[24:29]:\n",
    "    print(f'The {i} column is {all_cycling_collisions[i].isna().sum() / len(all_cycling_collisions) * 100}% NaNs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a sharp decline in the number of rows with vehicle codes going from the second to the third column. The string values, and the NaN values will be dealt with in the data preparation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "### 2.1 Drop Unnecessary Columns\n",
    "\n",
    "The first step of preprocessing is to drop the columns that are not useful for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43279, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unwanted columns using this list\n",
    "all_cycling_collisions = all_cycling_collisions.drop([ 'NUMBER OF PERSONS INJURED',\n",
    "                                                       'NUMBER OF PERSONS KILLED', \n",
    "                                                       'NUMBER OF PEDESTRIANS INJURED',\n",
    "                                                       'NUMBER OF PEDESTRIANS KILLED',  \n",
    "                                                       'NUMBER OF MOTORIST INJURED',\n",
    "                                                       'NUMBER OF MOTORIST KILLED',\n",
    "                                                       'COLLISION_ID' ] , axis=1)\n",
    "\n",
    "# Making sure it worked\n",
    "all_cycling_collisions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Making the 'Lethal' Column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can seperate our training data and our holdout data, we need to make a target y column, as our two datasets currently store this information in two seperate columns. After we have created this new column, we can drop the original two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    43101\n",
       "1      178\n",
       "Name: Lethal, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the lethality column and filling it with zeros\n",
    "all_cycling_collisions['Lethal'] = 0\n",
    "\n",
    "# Going through the data frame and marking lethal collisions\n",
    "all_cycling_collisions.loc[ all_cycling_collisions['NUMBER OF CYCLIST KILLED'] > 0,['Lethal']] = 1\n",
    "\n",
    "# Dropping the old columns\n",
    "all_cycling_collisions = all_cycling_collisions.drop(['NUMBER OF CYCLIST KILLED', 'NUMBER OF CYCLIST INJURED'], axis=1)\n",
    "\n",
    "# The all-important sanity check\n",
    "all_cycling_collisions['Lethal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collision Year \n",
    "\n",
    "One of the features we want to include in our modelling process is what year the collision took place in, to do this we are going to create a new column for year, and make sure it is in a format that can be handled by SciKit-learn's OneHotEncoder( ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020    5481\n",
       "2019    4964\n",
       "2016    4945\n",
       "2017    4864\n",
       "2018    4694\n",
       "2015    4266\n",
       "2013    4062\n",
       "2014    3999\n",
       "2021    3802\n",
       "2012    2202\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our year column, so we can One-Hot Encode it in a pipeline later on. \n",
    "all_cycling_collisions['Year'] = [int(date[-4:]) for date in all_cycling_collisions['CRASH DATE']]\n",
    "\n",
    "# Checking our work \n",
    "all_cycling_collisions['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Collision Month\n",
    "\n",
    "We are creating this column for two reasons. The first is that this information could be a very good predictor of cycling lethality, for example if icy roads contribute to cyclist deaths in the winter months. The other reason is that our models are going to need every feature we can give them if they are going to be able to identify the minority class in spite of the extreme class imbalance. Also fortunate is the fact that every single record in our dataset has an entry for 'CRASH DATE', so we dont have to deal with any NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     5768\n",
       "7     5539\n",
       "9     5420\n",
       "6     4819\n",
       "10    4277\n",
       "5     3998\n",
       "11    3068\n",
       "4     2817\n",
       "3     2193\n",
       "12    2146\n",
       "1     1725\n",
       "2     1509\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our 'Month' Column \n",
    "all_cycling_collisions['Month'] = [int(date[:2]) for date in all_cycling_collisions['CRASH DATE']]\n",
    "\n",
    "# Checking the values \n",
    "all_cycling_collisions['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Holiday Column\n",
    "\n",
    "To take further advantage of the lack of NaN values, we are going to engineer a Holiday Column, that tracks whether or not a collision took place on or very near to a major holiday. Unfortunately, a holiday means a predictable spike in drunk driving, and also just an increase in travel in general. This column will track collisions that took place on the most hazardous holiday travel days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41163\n",
       "1     2116\n",
       "Name: Holiday Collision, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chopping the year off of the date strings for easier syntax later\n",
    "all_cycling_collisions['CRASH DATE'] = [date[:5] for date in all_cycling_collisions['CRASH DATE']]\n",
    "\n",
    "# Making a list of the relevant dates\n",
    "holiday_travel_dates = ['12/31', '01/01',\n",
    "                        '05/27', '05/28', '05/29', '05/30',\n",
    "                        '06/03', '06/04' , '06/05',\n",
    "                        '09/05', '09/06', '09/07', \n",
    "                        '11/24', '11/25', '11/26',\n",
    "                        '12/24', '12/25', '12/26',\n",
    "                        '10/31', '11/1']\n",
    "\n",
    "# Creating the column and filling it with zeros to begin with\n",
    "all_cycling_collisions['Holiday Collision'] = 0\n",
    "\n",
    "# Marking the relevant collisions\n",
    "for date in holiday_travel_dates:\n",
    "    all_cycling_collisions.loc[all_cycling_collisions['CRASH DATE'] == date, ['Holiday Collision']] = 1\n",
    "    \n",
    "# With the last of our 'CRASH DATE' based feature engineering done, we can drop it from our dataframe\n",
    "all_cycling_collisions = all_cycling_collisions.drop(['CRASH DATE'], axis=1)\n",
    "    \n",
    "# Sanity check    \n",
    "all_cycling_collisions['Holiday Collision'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time of Day \n",
    "\n",
    "The hour of the day the accident occured in could have big implications in our model, and for our business problem. As it stands right now, the 'CRASH TIME' column specifies the time down to the minute, which is a more granular breakdown than our model requires. We will convert the hour and minute information into only what hour the accident occured in. Thankfully, the data is already in a 24 hour format, and there are no NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18    3588\n",
       "17    3558\n",
       "19    3306\n",
       "16    3188\n",
       "20    2932\n",
       "14    2792\n",
       "15    2715\n",
       "13    2449\n",
       "21    2394\n",
       "12    2123\n",
       "22    1825\n",
       "11    1823\n",
       "9     1634\n",
       "8     1562\n",
       "10    1477\n",
       "23    1342\n",
       "0     1307\n",
       "7      961\n",
       "6      569\n",
       "1      514\n",
       "2      365\n",
       "5      312\n",
       "3      279\n",
       "4      264\n",
       "Name: Hour, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting just the hour information and storing it in a new column, 'Hour'\n",
    "all_cycling_collisions['Hour'] = [int(time[:2].replace(':','')) for time in all_cycling_collisions['CRASH TIME']]\n",
    "\n",
    "# Dropping the 'CRASH TIME' column, as we no longer need it\n",
    "all_cycling_collisions = all_cycling_collisions.drop(['CRASH TIME'], axis=1)\n",
    "\n",
    "# Sanity Check\n",
    "all_cycling_collisions['Hour'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Borough\n",
    "\n",
    "Borough could turn out to be a very useful feature for our classification models, but first we have to contend with all the NaN values in that column. We know from our earlier NaN report on this column that roughly a quarter of collisions do not specify what Borough the crash took place in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROOKLYN         13425\n",
      "MANHATTAN         9841\n",
      "QUEENS            6698\n",
      "BRONX             3496\n",
      "STATEN ISLAND      403\n",
      "Name: BOROUGH, dtype: int64\n",
      "\n",
      "        There are 9416 rows with a NaN for Borough\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the values in the Borough Column \n",
    "print(all_cycling_collisions[\"BOROUGH\"].value_counts())\n",
    "\n",
    "# This dataframe is all of our rows where Borough holds a NaN value\n",
    "nan_boro = all_cycling_collisions.loc[all_cycling_collisions['BOROUGH'].isna()]\n",
    "\n",
    "# This line confirms what our NaN report told us earlier, by telling us the amount of NaN's\n",
    "print(f'''\n",
    "        There are {len(nan_boro)} rows with a NaN for Borough''')\n",
    "\n",
    "# This line prints how many of our 9416 rows have latitude and longitude data\n",
    "len(nan_boro) - nan_boro['LOCATION'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows us that even though we have 9416 rows that dont specify Borough, 6966 of those rows do specify Latitude and Longitude data, which we can use to extrapolate what Borough they occured in. (Street name might be easier, or put the zip code thing before this and then use them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by renaming the column to match the others\n",
    "all_cycling_collisions['Borough'] = all_cycling_collisions['BOROUGH']\n",
    "all_cycling_collisions = all_cycling_collisions.drop(['BOROUGH'], axis=1)\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Zip Code \n",
    "\n",
    "We definitely want to feed this feature into our model, as it could help our classificaiton algorithm perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2450"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how many rows do not contain Zip Code information\n",
    "print(sum(all_cycling_collisions['ZIP CODE'].isna()))\n",
    "\n",
    "# Of those 9417 rows that dont have zipcode data, only 2450 also do not have latitude and longitude \n",
    "len(all_cycling_collisions.loc[all_cycling_collisions['ZIP CODE'].isna() & all_cycling_collisions['LOCATION'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above numbers mean is that if we can find a way to get the zipcode from the Latitude and Longitude, we can fill in three quarters of our missing zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename our column so we can tell it's been processed \n",
    "all_cycling_collisions['Zip Code'] = [zipcode for zipcode in all_cycling_collisions['ZIP CODE'] ]\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# ZIP CODE WIZARDY GOES HERE, STACK OVERFLOW PAGE HAS BEEN BOOKMARKED\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# Now that we are finished with the Location-Based columns, we can drop the originals \n",
    "all_cycling_collisions = all_cycling_collisions.drop(['ZIP CODE', 'LATITUDE', 'LONGITUDE', 'LOCATION'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Street Name\n",
    "\n",
    "If it becomes a strong predictor, street name could be an essential feature for our business problem, as it would tell the city which streets are the most dangerous for cyclists, and therefore where to focus their efforts in terms of building new infrastructure. Street name in our dataset comes in the form of three columns, ON STREET NAME, CROSS STREET NAME, and OFF STREET NAME. The values in these columns are fairly clean in their formatting, and dont exhibit any significant overlap or redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 rows without any street name data.\n"
     ]
    }
   ],
   "source": [
    "# This dataframe contains the rows with no street data whatsoever\n",
    "no_street_data = all_cycling_collisions.loc[all_cycling_collisions['ON STREET NAME'].isna() & \n",
    "                                            all_cycling_collisions['CROSS STREET NAME'].isna() &\n",
    "                                            all_cycling_collisions['OFF STREET NAME'].isna()]\n",
    "\n",
    "# Luckily only 61 rows of our entire dataframe contain zero street name values\n",
    "print(f'There are {len(no_street_data)} rows without any street name data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finding the columns with NaN values under Cross Street, and filling them in with the values from OFF STREET NAME, if there are any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11826\n",
      "5681\n"
     ]
    }
   ],
   "source": [
    "# Renaming our two columns \n",
    "all_cycling_collisions['Street'] = all_cycling_collisions['ON STREET NAME']\n",
    "all_cycling_collisions['Cross Street'] = all_cycling_collisions['CROSS STREET NAME']\n",
    "\n",
    "# Check how many NaN values we have in Cross Street before meshing it with OFF STREET NAME\n",
    "print(len(all_cycling_collisions.loc[all_cycling_collisions['Cross Street'].isna()]))\n",
    "\n",
    "# For our columns without a listed Cross Street, we are using the OFF STREET NAME value, if it has one\n",
    "all_cycling_collisions.loc[all_cycling_collisions['Cross Street'].isna(), ['Cross Street']] = all_cycling_collisions['OFF STREET NAME']\n",
    "\n",
    "# Number of NaNs in Cross Street after the merge\n",
    "print(len(all_cycling_collisions.loc[all_cycling_collisions['Cross Street'].isna()]))\n",
    "\n",
    "# Drop the old columns \n",
    "all_cycling_collisions = all_cycling_collisions.drop(['ON STREET NAME','CROSS STREET NAME','OFF STREET NAME'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Contributing Factor Columns \n",
    "\n",
    "Our strategy for these columns is to use the values in the first column as the defaults, then for the rows where the contributing factor is unknown, use the values from the second column to fill in some of the gaps. Because the 3rd, 4th and 5th columns are almost entirely NaN values, we will ignore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Factor 1 column using CONTRIBUTING FACTOR VEHICLE 1 as the default value\n",
    "all_cycling_collisions['Factor 1'] = all_cycling_collisions['CONTRIBUTING FACTOR VEHICLE 1']\n",
    "\n",
    "# Going through Factor 1, and any rows that equal 'Unspecified', we input the value of CONTRIBUTING FACTOR VEHICLE 2\n",
    "all_cycling_collisions.loc[all_cycling_collisions['Factor 1'] == 'Unspecified', ['Factor 1']] = all_cycling_collisions['CONTRIBUTING FACTOR VEHICLE 2']\n",
    "\n",
    "# Dropping the old columns\n",
    "all_cycling_collisions.drop(['CONTRIBUTING FACTOR VEHICLE 1', \n",
    "                             'CONTRIBUTING FACTOR VEHICLE 2',\n",
    "                             'CONTRIBUTING FACTOR VEHICLE 3', \n",
    "                             'CONTRIBUTING FACTOR VEHICLE 4',\n",
    "                             'CONTRIBUTING FACTOR VEHICLE 5'], \n",
    "                                 axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Vehicle Type Codes \n",
    "\n",
    "Unlike some of the other features in this dataset, the Vehicle Type Codes are horrific in their messy, overlapping nature. For example 'VAN', 'Van, and 'van' are all seperate categories. To deal with this we have created a few buckets to sort each string into, that will hopefully give our model more insight into what vehicle type makes for a deadly crash. Each of these new cleaned categories will be a single column, 'Vehicle Type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sedan                   18302\n",
       "SUV                     11518\n",
       "Vehicle Type Unknown     6431\n",
       "Taxi                     3673\n",
       "Scooter                  1093\n",
       "Pickup                    647\n",
       "Work Van                  628\n",
       "Bus                       557\n",
       "Large Truck               430\n",
       "Name: Vehicle Type, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our column with a default 'Vehicle Type Unknown' string value\n",
    "all_cycling_collisions['Vehicle Type'] = 'Vehicle Type Unknown'\n",
    "\n",
    "# Making a list of our vehicle type columns, so we can iterate over them\n",
    "vehicle_columns = ['VEHICLE TYPE CODE 1',\n",
    "           'VEHICLE TYPE CODE 2',\n",
    "           'VEHICLE TYPE CODE 3', \n",
    "           'VEHICLE TYPE CODE 4',\n",
    "           'VEHICLE TYPE CODE 5']\n",
    "\n",
    "# This dictionary translates the most common vehicle codes into our new categories    \n",
    "dict = {'Sedan':'Sedan','4 dr sedan':'Sedan','2 dr sedan': 'Sedan', 'Convertible':'Sedan','SEDAN':'Sedan',\n",
    "        'PASSENGER VEHICLE':'Sedan', 'Station Wagon/Sport Utility Vehicle':'SUV', \n",
    "        'SPORT UTILITY / STATION WAGON':'SUV', 'Taxi': 'Taxi', 'TAXI':'Taxi', \n",
    "        'LIVERY VEHICLE': 'Taxi', 'VAN':'Work Van', 'SMALL COM VEH':'Work Van', 'Van':'Work Van', 'van':'Work Van', \n",
    "        'Pick-up Truck':'Pickup', 'PICK-UP TRUCK':'Pickup', 'Box Truck':'Large Truck', 'LARGE COM VEH':'Large Truck',\n",
    "        'Tractor Truck Diesel':'Large Truck','Flat Bed':'Large Truck','Tow Truck / Wrecker':'Large Truck', \n",
    "        'Tractor Truck Gasoline':'Large Truck','Concrete Mixer':'Large Truck','FIRE TRUCK':'Large Truck',\n",
    "        'Armored Truck':'Large Truck','TRUCK':'Large Truck','FIRE':'Large Truck', 'E-Scooter':'Scooter',\n",
    "        'E-Sco':'Scooter', 'Moped':'Scooter','SCOOTER':'Scooter','DELIV':'Scooter', 'Bus':'Bus','BUS':'Bus',\n",
    "        'School Bus':'Bus', 'Garbage or Refuse':'Large Truck', 'Tanker':'Large Truck'}\n",
    "\n",
    "# Iterating over our dictionary by key\n",
    "for i in dict.keys():\n",
    "\n",
    "    # Iterating over each vehicle column\n",
    "    for j in vehicle_columns:\n",
    "    \n",
    "        all_cycling_collisions.loc[all_cycling_collisions[j] == i, ['Vehicle Type']] = dict[i]\n",
    "    \n",
    "# Dropping the old columns\n",
    "all_cycling_collisions.drop(['VEHICLE TYPE CODE 1',\n",
    "                             'VEHICLE TYPE CODE 2', \n",
    "                             'VEHICLE TYPE CODE 3', \n",
    "                             'VEHICLE TYPE CODE 4',\n",
    "                             'VEHICLE TYPE CODE 5'], \n",
    "                              axis=1, inplace=True)\n",
    "\n",
    "# Checking out the final result\n",
    "all_cycling_collisions['Vehicle Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lethal</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Holiday Collision</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Street</th>\n",
       "      <th>Cross Street</th>\n",
       "      <th>Factor 1</th>\n",
       "      <th>Vehicle Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>11368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-10    ROOSEVELT AVENUE</td>\n",
       "      <td>Turning Improperly</td>\n",
       "      <td>SUV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BATH AVENUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Failure to Yield Right-of-Way</td>\n",
       "      <td>SUV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10452</td>\n",
       "      <td>GRANT HIGHWAY</td>\n",
       "      <td>UNIVERSITY AVENUE</td>\n",
       "      <td>Pedestrian/Bicyclist/Other Pedestrian Error/Co...</td>\n",
       "      <td>SUV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11201</td>\n",
       "      <td>FLATBUSH AVENUE EXTENSION</td>\n",
       "      <td>JOHNSON STREET</td>\n",
       "      <td>Driver Inattention/Distraction</td>\n",
       "      <td>Sedan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>11369</td>\n",
       "      <td>93 STREET</td>\n",
       "      <td>32 AVENUE</td>\n",
       "      <td>Unsafe Speed</td>\n",
       "      <td>Sedan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Lethal  Year  Month  Holiday Collision  Hour   Borough Zip Code  \\\n",
       "52        0  2021      4                  0    11    QUEENS    11368   \n",
       "90        0  2021      4                  0     0       NaN      NaN   \n",
       "139       0  2021      4                  0    17     BRONX    10452   \n",
       "145       0  2021      4                  0    19  BROOKLYN    11201   \n",
       "178       0  2021      4                  0     0    QUEENS    11369   \n",
       "\n",
       "                        Street                Cross Street  \\\n",
       "52                         NaN  100-10    ROOSEVELT AVENUE   \n",
       "90                 BATH AVENUE                         NaN   \n",
       "139              GRANT HIGHWAY           UNIVERSITY AVENUE   \n",
       "145  FLATBUSH AVENUE EXTENSION              JOHNSON STREET   \n",
       "178                  93 STREET                   32 AVENUE   \n",
       "\n",
       "                                              Factor 1 Vehicle Type  \n",
       "52                                  Turning Improperly          SUV  \n",
       "90                       Failure to Yield Right-of-Way          SUV  \n",
       "139  Pedestrian/Bicyclist/Other Pedestrian Error/Co...          SUV  \n",
       "145                     Driver Inattention/Distraction        Sedan  \n",
       "178                                       Unsafe Speed        Sedan  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at our cleaned dataset\n",
    "all_cycling_collisions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Train Holdout Split, and Train Test Split\n",
    "\n",
    "The absolute first thing we need to do in terms of preprocessing is to seperate our data into the train and holdout set to avoid data leakage, and make sure we can robustly evaluate our models later on. Then we seperate our training data further into train and test so we can evaluate different model iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seperating our data into features and target\n",
    "X = all_cycling_collisions.drop(['Lethal'], axis=1)\n",
    "y = all_cycling_collisions['Lethal']\n",
    "\n",
    "# Train holdout split\n",
    "X_training, X_hold, y_training, y_hold = train_test_split(X, y, random_state=seed, test_size=.10)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, random_state=seed, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Imputing Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This line will hold our column names\n",
    "\n",
    "columns = {0:'Year', 1:'Month', 2:'Holiday Collision', 3:'Hour', 4:'Borough', 5:'Zip Code', \n",
    " 6:'Street', 7:'Cross Street', 8:'Factor 1', 9:'Vehicle Type'}\n",
    "# Imputing NaN values\n",
    "imp = SimpleImputer(strategy='constant')\n",
    "imp.fit(X_train)\n",
    "X_train_imputed = pd.DataFrame(imp.transform(X_train))\n",
    "\n",
    "imp = SimpleImputer(strategy='constant')\n",
    "imp.fit(X_test)\n",
    "X_test_imputed = pd.DataFrame(imp.transform(X_test))\n",
    "\n",
    "# Adding the original column names back on \n",
    "X_train_imputed.rename(columns=columns, inplace=True)\n",
    "X_test_imputed.rename(columns=columns, inplace=True)\n",
    "\n",
    "\n",
    "one_hots = ['Year', 'Month', 'Holiday Collision', 'Hour', 'Borough','Vehicle Type']\n",
    "ordinals = ['Zip Code', 'Street', 'Cross Street', 'Factor 1' ]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False), one_hots),\n",
    "    ('ord', OrdinalEncoder(), ordinals)\n",
    "])\n",
    "\n",
    "\n",
    "X_train_encoded =  pd.DataFrame(ct.fit_transform(X_train_imputed))\n",
    "\n",
    "X_test_encoded = pd.DataFrame(ct.fit_transform(X_test_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_encoded))\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train_encoded, y_train) \n",
    "\n",
    "print(len(X_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling \n",
    "\n",
    "### 4.1 Dummy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiating our Dummy Model and fitting it to our training data\n",
    "clf_1 = DummyClassifier(strategy='most_frequent')\n",
    "clf_1.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Looking at the confusion matrix\n",
    "plot_confusion_matrix(clf_1, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Baseline Model - Logistic Regression \n",
    "\n",
    "   This will serve as our baseline model to compare the performance of our later model iterations to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_2 = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "clf_2.fit(X_train_encoded, y_train)\n",
    "cross_val_score(clf_2, X_train_encoded, y_train, scoring='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a grid search and see if that helps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pg2 = {'tol' : [1e-4, 1e-5, 1e-6, 1e-7],\n",
    "       'C' : [.1, .01, .001],\n",
    "       'max_iter': [500 , 1000]}\n",
    "\n",
    "# The grid search is commented out to reduce notebook runtime, best parameters are input into best_logreg\n",
    "\n",
    "# logreg = LogisticRegression(random_state=seed)\n",
    "# tuned_logreg = GridSearchCV(logreg, pg2, cv=5, scoring='recall')\n",
    "# tuned_logreg.fit(X_train_encoded, y_train)\n",
    "# tuned_logreg.best_estimator_\n",
    "# plot_confusion_matrix(tuned_logreg, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_logreg = LogisticRegression(random_state=seed, C=0.1, max_iter=1000000)\n",
    "\n",
    "tuned_logreg.fit(X_train_resampled, y_train_resampled)\n",
    "plot_confusion_matrix(tuned_logreg, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_logreg2 = LogisticRegression(random_state=seed,class_weight='balanced', C=0.1, max_iter=1000000)\n",
    "\n",
    "tuned_logreg2.fit(X_train_encoded, y_train)\n",
    "plot_confusion_matrix(tuned_logreg2, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 K-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf_3 = KNeighborsClassifier()\n",
    "clf_3.fit(X_train_encoded, y_train)\n",
    "plot_confusion_matrix(clf_3, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg3 = {'n_neighbors': [1, 3, 5, 11],\n",
    "       'weights': ['uniform', 'distance'],\n",
    "       'leaf_size': [ 50, 10, 1]}\n",
    "\n",
    "# The grid search is commented out to reduce notebook runtime, best parameters are input into tuned_knn\n",
    "\n",
    "# knn = GridSearchCV(clf_3, pg3, cv=5, scoring='recall')\n",
    "# knn.fit(X_train_encoded, y_train)\n",
    "\n",
    "tuned_knn = (KNeighborsClassifier(n_neighbors = 1, leaf_size=10, weights='distance')).fit(X_train_encoded, y_train)\n",
    "\n",
    "plot_confusion_matrix(tuned_knn, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_5 = DecisionTreeClassifier(random_state=seed)\n",
    "clf_5.fit(X_train_encoded, y_train)\n",
    "cross_val_score(clf_5, X_train_encoded, y_train, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pg4 = {'criterion': ['gini'],\n",
    "       'min_samples_split': [2],\n",
    "       'min_samples_leaf': [1]}\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=seed)\n",
    "tuned_tree = GridSearchCV(tree, pg4, cv=5, scoring='recall')\n",
    "tuned_tree.fit(X_train_resampled, y_train_resampled)\n",
    "#tree_tuner.best_params_\n",
    "\n",
    "\n",
    "# tuned_tree.fit(X_train_encoded, y_train)\n",
    "plot_confusion_matrix(tuned_tree, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_6 = RandomForestClassifier(random_state=seed)\n",
    "clf_6.fit(X_train_encoded, y_train)\n",
    "cross_val_score(clf_6, X_train_encoded, y_train, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg5 = {'n_estimators' : [10, 100],\n",
    "       'min_samples_split': [2],\n",
    "       'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# forest = RandomForestClassifier(random_state=seed)\n",
    "# forest_tuner = GridSearchCV(forest, pg5, cv=5, scoring='recall')\n",
    "# forest_tuner.fit(X_train_resampled, y_train_resampled)\n",
    "# forest_tuner.best_params_  \n",
    "\n",
    "# plot_confusion_matrix(forest_tuner, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_forest = RandomForestClassifier(\n",
    "                                      min_samples_leaf= 5,\n",
    "                                      min_samples_split= 2,\n",
    "                                      n_estimators= 100)\n",
    "\n",
    "# print(cross_val_score(tuned_forest, X_train_encoded, y_train, scoring='accuracy'))\n",
    "# print(cross_val_score(tuned_forest, X_train_encoded, y_train, scoring='recall'))\n",
    "\n",
    "tuned_forest.fit(X_train_resampled, y_train_resampled)\n",
    "plot_confusion_matrix(tuned_forest, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
